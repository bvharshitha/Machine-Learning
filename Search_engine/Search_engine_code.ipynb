{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "jgTvQKavfLtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfad9901-4ce7-4c11-ec3c-f67a28182752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Dataset"
      ],
      "metadata": {
        "id": "Wk4q7S27e1HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_documents(corpusroot):\n",
        "    documents = []\n",
        "    names = []\n",
        "\n",
        "    for filename in os.listdir(corpusroot):\n",
        "        if filename.endswith(\".txt\"):  # Check if the file is a .txt file\n",
        "            file_path = os.path.join(corpusroot, filename)\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding='windows-1252') as file:\n",
        "                    doc = file.read().lower()  # Convert to lowercase\n",
        "                    documents.append(doc)\n",
        "                    names.append(filename)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return documents, names\n"
      ],
      "metadata": {
        "id": "VcZSA4qjfByn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing Corupus"
      ],
      "metadata": {
        "id": "kclvUC9pe6Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    tokenized_corpus = []\n",
        "    for doc in corpus:\n",
        "        tokens = nltk.word_tokenize(doc)  # Tokenize the document\n",
        "        tokenized_corpus.append(tokens)\n",
        "    return tokenized_corpus"
      ],
      "metadata": {
        "id": "Oym83x5SfRG_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StopWords Removal from Corups"
      ],
      "metadata": {
        "id": "E-dY8KyNfFjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def filterstopwords(tokenized_corpus):\n",
        "    cleaned_corpus = []\n",
        "    for tokens in tokenized_corpus:\n",
        "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove stopwords\n",
        "        cleaned_corpus.append(filtered_tokens)\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "EjwNUZQmf54G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n"
      ],
      "metadata": {
        "id": "ZA_JLgHmfOj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(cleaned_corpus):\n",
        "    stemmed_corpus = []\n",
        "    for tokens in cleaned_corpus:\n",
        "        stemmed_tokens = [stemmer.stem(word) for word in tokens]  # Stem the tokens\n",
        "        stemmed_corpus.append(stemmed_tokens)\n",
        "    return stemmed_corpus"
      ],
      "metadata": {
        "id": "nsJweduUf-oW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the Documents"
      ],
      "metadata": {
        "id": "aRazluSRfSYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process all documents\n",
        "files = './US_Inaugural_Addresses'\n",
        "corpus = []\n",
        "doc_names = []\n",
        "\n",
        "def preprocessing(files):\n",
        "    documents, names = read_documents(files)\n",
        "    tokenized_doc = tokenize(documents)\n",
        "    filtered_doc = filterstopwords(tokenized_doc)\n",
        "    stemmed_corpus = stemming(filtered_doc)\n",
        "\n",
        "    return stemmed_corpus, names\n",
        "\n",
        "\n",
        "corpus, doc_names = preprocessing(files)\n"
      ],
      "metadata": {
        "id": "aCeJouWwguZi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing TF*IDF"
      ],
      "metadata": {
        "id": "9sVuJ9kBfZgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Compute Document Frequency (DF)\n",
        "def df(corpus):\n",
        "    df_dict = defaultdict(int)\n",
        "    for doc in corpus:\n",
        "        unique_tokens = set(doc)\n",
        "        for token in unique_tokens:\n",
        "            df_dict[token] += 1\n",
        "    return df_dict\n",
        "\n",
        "df_dict = df(corpus)\n",
        "N = len(corpus)\n",
        "\n",
        "# 2. Function to calculate IDF for a term\n",
        "def idf(term, df_dict, N):\n",
        "    \"\"\"Compute the IDF for a given term.\"\"\"\n",
        "    if term in df_dict:\n",
        "        return math.log10(N / df_dict[term])\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "# 3. Compute Term Frequency (TF)\n",
        "def tf(token_list):\n",
        "    tf_dict = defaultdict(int)\n",
        "    for token in token_list:\n",
        "        tf_dict[token] += 1\n",
        "    return tf_dict\n",
        "\n",
        "# 4. Compute TF-IDF for a document\n",
        "def tfidf(tf_dict, df_dict, N):\n",
        "    tfidf_dict = {}\n",
        "    for token, tf in tf_dict.items():\n",
        "        term_idf = idf(token, df_dict, N)\n",
        "        tfidf_dict[token] = (1 + math.log10(tf)) * term_idf\n",
        "    return tfidf_dict\n",
        "\n",
        "# 5. Compute and normalize TF-IDF for the entire corpus\n",
        "def tfidf_corpus(corpus, df_dict, N):\n",
        "    corpus_tfidf = []\n",
        "    for doc_tokens in corpus:\n",
        "        term_freq = tf(doc_tokens)\n",
        "        tfidf_vector =tfidf(term_freq, df_dict, N)\n",
        "        # Normalize the TF-IDF vector\n",
        "        norm_tfidf = {key: value / math.sqrt(sum(v ** 2 for v in tfidf_vector.values())) for key, value in tfidf_vector.items()}\n",
        "        corpus_tfidf.append(norm_tfidf)\n",
        "    return corpus_tfidf\n",
        "\n",
        "corpus_tfidf = tfidf_corpus(corpus, df_dict, N)\n",
        "\n",
        "# 6. Function to get IDF of a term\n",
        "def getidf(term):\n",
        "    term_stemmed = stemmer.stem(term.lower())\n",
        "    return idf(term_stemmed, df_dict, N)\n",
        "\n",
        "# 7. Function to get TF-IDF weight of a term in a specific document\n",
        "def getweight(doc_name, term):\n",
        "    try:\n",
        "        # Find the index of the document in the doc_names list\n",
        "        doc_index = doc_names.index(doc_name)\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Document '{doc_name}' not found in doc_names list.\")\n",
        "\n",
        "    term_stemmed = stemmer.stem(term.lower())\n",
        "\n",
        "    # Look up the TF-IDF weight for the term in the document's TF-IDF vector\n",
        "    if term_stemmed in corpus_tfidf[doc_index]:\n",
        "        return corpus_tfidf[doc_index][term_stemmed]\n",
        "    else:\n",
        "        return 0.0\n",
        "\n"
      ],
      "metadata": {
        "id": "1C_0Yywb1Mlt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query Vector"
      ],
      "metadata": {
        "id": "rqENS2sUj8IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_query_vector(qstring):\n",
        "    tokens = nltk.word_tokenize(qstring.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    stemmed = [stemmer.stem(word) for word in tokens]\n",
        "    query_tf = tf(stemmed)\n",
        "    query_weights = {token: (1 + math.log10(tf)) for token, tf in query_tf.items()}\n",
        "    norm = math.sqrt(sum(weight ** 2 for weight in query_weights.values()))\n",
        "    return {token: weight / norm for token, weight in query_weights.items()}\n"
      ],
      "metadata": {
        "id": "WBFv4nMMgVfT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity"
      ],
      "metadata": {
        "id": "T9c_QXHKj3vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum(vec1.get(k, 0) * vec2.get(k, 0) for k in vec1.keys())\n",
        "    norm1 = math.sqrt(sum(val ** 2 for val in vec1.values()))\n",
        "    norm2 = math.sqrt(sum(val ** 2 for val in vec2.values()))\n",
        "\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return dot_product / (norm1 * norm2)"
      ],
      "metadata": {
        "id": "umaWFUJagXz_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bulding posting list\n"
      ],
      "metadata": {
        "id": "h_rmqi2kjyMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def posting_list(token):\n",
        "    postings = []\n",
        "    for doc_id, tfidf_vector in enumerate(corpus_tfidf):\n",
        "        if token in tfidf_vector:\n",
        "            weight = tfidf_vector[token]\n",
        "            postings.append((doc_id, weight))\n",
        "\n",
        "    return postings\n"
      ],
      "metadata": {
        "id": "co9wWa7ugfmG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query function"
      ],
      "metadata": {
        "id": "Plw0gq7Wjkhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def query(qstring, posting_limit=10):\n",
        "    query_vector = compute_query_vector(qstring)\n",
        "    actual_scores = defaultdict(float)\n",
        "\n",
        "    found_valid_document = False\n",
        "    token_present_corpus = False\n",
        "\n",
        "    for token, q_weight in query_vector.items():\n",
        "        # Build the postings list for this token\n",
        "        postings = posting_list(token)\n",
        "\n",
        "        # If postings are found for this token, mark that a token was found in the corpus\n",
        "        if len(postings) > 0:\n",
        "            token_present_corpus = True\n",
        "\n",
        "        if not token_present_corpus:\n",
        "            continue\n",
        "\n",
        "        top_postings = sorted(postings, key=lambda x: -x[1])[:posting_limit]\n",
        "\n",
        "        # Calculate scores for documents in the top postings\n",
        "        for doc_id, doc_weight in top_postings:\n",
        "            doc_vector = corpus_tfidf[doc_id]\n",
        "            similarity = cosine_similarity(query_vector, doc_vector)\n",
        "            actual_scores[doc_id] += similarity * q_weight\n",
        "            if similarity > 0:\n",
        "                found_valid_document = True  # Mark that we found a valid document\n",
        "\n",
        "    # If no tokens were found in the corpus, return \"None\"\n",
        "    if not token_present_corpus:\n",
        "        return \"None\", 0.0\n",
        "\n",
        "    # If no valid document was found in the top posting_list, return \"fetch more\"\n",
        "    if not found_valid_document:\n",
        "        return \"fetch more\", 0.0\n",
        "\n",
        "    # If we have valid scores, return the best document\n",
        "    if actual_scores:\n",
        "        best_doc = max(actual_scores, key=actual_scores.get)\n",
        "        return doc_names[best_doc], actual_scores[best_doc]\n",
        "\n",
        "\n",
        "    return \"None\", 0.0"
      ],
      "metadata": {
        "id": "gtaOl8Qug7Oh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%.12f\" % getidf('democracy'))\n",
        "print(\"%.12f\" % getidf('foreign'))\n",
        "print(\"%.12f\" % getidf('states'))\n",
        "print(\"%.12f\" % getidf('honor'))\n",
        "print(\"%.12f\" % getidf('great'))\n",
        "print(\"--------------\")\n",
        "print(\"%.12f\" % getweight('19_lincoln_1861.txt','constitution'))\n",
        "print(\"%.12f\" % getweight('23_hayes_1877.txt','public'))\n",
        "print(\"%.12f\" % getweight('25_cleveland_1885.txt','citizen'))\n",
        "print(\"%.12f\" % getweight('09_monroe_1821.txt','revenue'))\n",
        "print(\"%.12f\" % getweight('37_roosevelt_franklin_1933.txt','leadership'))\n",
        "print(\"--------------\")\n",
        "print(\"(%s, %.12f)\" % query(\"states laws\"))\n",
        "print(\"(%s, %.12f)\" % query(\"war offenses\"))\n",
        "print(\"(%s, %.12f)\" % query(\"british war\"))\n",
        "print(\"(%s, %.12f)\" % query(\"texas government\"))\n",
        "print(\"(%s, %.12f)\" % query(\"world civilization\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmQbv-pFoFIA",
        "outputId": "7b99035c-02c2-48ec-aa5e-bff400260c9f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.698970004336\n",
            "0.187086643357\n",
            "0.057991946978\n",
            "0.139661993429\n",
            "0.033858267261\n",
            "--------------\n",
            "0.006540919067\n",
            "0.008269301122\n",
            "0.011832819908\n",
            "0.027778670583\n",
            "0.077574191456\n",
            "--------------\n",
            "(21_grant_1869.txt, 0.025605903046)\n",
            "(20_lincoln_1865.txt, 0.180140227144)\n",
            "(07_madison_1813.txt, 0.123636547202)\n",
            "(15_polk_1845.txt, 0.052967825658)\n",
            "(22_grant_1873.txt, 0.014834244680)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRH1Jcx01UDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}